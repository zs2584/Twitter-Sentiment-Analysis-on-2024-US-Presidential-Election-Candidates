{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/03 00:01:56 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/05/03 00:01:56 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/05/03 00:01:56 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/05/03 00:01:56 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import TimestampType, StructType, StructField, StringType\n",
    "\n",
    "window_size = \"10 seconds\"\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tweet sentiment analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"gs://6893_bigdata_isaac/largescale_data_processing/6889project/tweet1.csv\")\n",
    "\n",
    "# Convert the 'time' column into a TimestampType\n",
    "df = df.withColumn(\"time\", F.col(\"time\").cast(TimestampType()))\n",
    "\n",
    "\n",
    "candidate_array = [\"biden\", \"nikki haley\", \"ramaswamy\", \"trump\", \"asa hutchinson\", \"marianne williamson\"]\n",
    "\n",
    "data = [(candidate,) for candidate in candidate_array]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"candidate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame from the candidate array\n",
    "distinct_candidates = spark.createDataFrame(data, schema)\n",
    "\n",
    "distinct_windows = df.select(F.window(F.col(\"time\"), window_size).alias(\"window\")).distinct()\n",
    "\n",
    "# Create a DataFrame with all possible combinations of candidates and windows\n",
    "all_combinations = distinct_candidates.crossJoin(distinct_windows)\n",
    "\n",
    "# Group by 'candidate' and tumbling window based on the 'time' column\n",
    "grouped_df = df.groupBy(\n",
    "    F.col(\"candidate\"),\n",
    "    F.window(F.col(\"time\"), window_size)\n",
    ")\n",
    "\n",
    "# Calculate the mean value of the sentiment score for each group\n",
    "sentiment =  grouped_df.agg(\n",
    "    F.mean(\"tweet\").alias(\"sentiment\"),\n",
    "    F.count(\"tweet\").alias(\"tweet_count\")\n",
    ")\n",
    "\n",
    "result = all_combinations.join(sentiment, on=[\"candidate\", \"window\"], how=\"left_outer\")\n",
    "\n",
    "result = result.fillna({\"sentiment\": 0, \"tweet_count\": 0})\n",
    "\n",
    "df2 = spark.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"gs://6893_bigdata_isaac/largescale_data_processing/6889project/wordcount1.csv\")\n",
    "\n",
    "df2 = df2.select(\n",
    "    \"time\",\n",
    "    \"candidate\",\n",
    "    F.col(\"word\"),\n",
    "    F.col(\"count\").alias(\"word_count\")\n",
    ")\n",
    "\n",
    "# Convert the 'time' column of the new DataFrame into a TimestampType\n",
    "df2 = df2.withColumn(\"time\", F.col(\"time\").cast(TimestampType()))\n",
    "\n",
    "# Create a window column in the new DataFrame\n",
    "df2 = df2.withColumn(\"window\", F.window(F.col(\"time\"), window_size))\n",
    "\n",
    "final_result = result.join(df2, on=[\"candidate\", \"window\"], how=\"left_outer\")\n",
    "\n",
    "result = final_result.drop(\"window\", \"time\").orderBy([\"window.start\", \"candidate\"], ascending=[0, 1])\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of rows in the result DataFrame\n",
    "total_rows = result.count()\n",
    "\n",
    "# Calculate the number of DataFrames needed\n",
    "#num_dfs = (total_rows + 5) // 6\n",
    "num_dfs = 5\n",
    "\n",
    "# Split the result DataFrame into smaller DataFrames with 6 rows each\n",
    "smaller_dfs = []\n",
    "for i in range(num_dfs):\n",
    "    start_index = i * 6\n",
    "    end_index = start_index + 6\n",
    "    rows = result.take(end_index)[-6:]\n",
    "    # Create a smaller DataFrame with the fetched rows\n",
    "    smaller_df = spark.createDataFrame(rows, schema=result.schema)\n",
    "    smaller_dfs.append(smaller_df)\n",
    "\n",
    "# Show the smaller DataFrames\n",
    "for index, small_df in enumerate(smaller_dfs, start=1):\n",
    "    output_file = f\"gs://6893_bigdata_isaac/largescale_data_processing/6889project/result1/window_{index}.csv\"\n",
    "    small_df.toPandas().to_csv(output_file, index=None)\n",
    "    small_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sentiment_df = final_result.select(\n",
    "    \"candidate\",\n",
    "    \"window\",\n",
    "    \"sentiment\"\n",
    ")\n",
    "window_spec = Window.partitionBy(\"candidate\").orderBy(\"window\")\n",
    "\n",
    "sentiment_df = sentiment_df.withColumn(\"index\", row_number().over(window_spec))\n",
    "\n",
    "sentiment_df = sentiment_df.drop(\"window\")\n",
    "\n",
    "sentiment_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618703fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.toPandas().to_csv(\"gs://6893_bigdata_isaac/largescale_data_processing/6889project/result1/sentiment.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fe377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}